# Web Crawler Template

A configurable, modular Python web crawler framework.

---

## 🚀 Features

- **Modular Core**:  
  - `core/utils.py`: rate-limiting and retry decorators, logging setup  
  - `core/crawler.py`: `BaseCrawler` abstract class defining fetch/parse/save workflow  
  - `core/parser.py`: generic `extract_fields()` helper for CSS‐selector-based extraction  

- **Config-Driven**:  
  Use a simple YAML file to specify site name, URLs, CSS selectors, concurrency limits, rate limits, and output settings.

- **Example Implementations**:  
  - **AI Litigation Database**: `examples/ai_lit_config.yaml` + `examples/ai_lit_crawler.py`  
  - **Climate Litigation Database**: `examples/climate_lit_config.yaml` + `examples/climate_lit_crawler.py`

- **Easy Extension**:  
  Add a new site by copying the `config.yaml` template and writing a small subclass in `examples/`.

---

## 📦 Requirements

```text
pandas>=1.3,<2.0
selenium>=4.0,<5.0
beautifulsoup4>=4.9,<5.0
requests>=2.25,<3.0
PyYAML>=5.3,<6.0
```
Install with: `pip install -r requirements.txt`

---

## 🔧 Quick Start

- Clone the repository: 
     - `git clone https://github.com/<your-username>/web-crawler-template.git
cd web-crawler-template`
- Install dependencies:
     - `pip install -r requirements.txt`
- Run the example crawlers:
     - **AI Litigation Database**: `python run.py --config examples/ai_lit_config.yaml`
     - **Climate Litigation Database**: `python run.py --config examples/climate_lit_config.yaml`
-	Check your output:
     - data/ai_lit.csv
	   - data/climate_lit.xlsx

---

## 🔨 Adding a New Site

1.	Copy the template: `cp config.yaml my_site_config.yaml`
2.	Edit my_site_config.yaml  
Fill in your site’s name, list URL, selectors, pagination or load-more settings, and output path.	
3.	Implement a crawler subclass  
Create examples/my_site_crawler.py:
```
from core.crawler import BaseCrawler
from core.parser import extract_fields
from bs4 import BeautifulSoup
import requests

class MySiteCrawler(BaseCrawler):
    def fetch_list(self):
        # Return a list of raw HTML strings or request parameters
        ...

    def parse_list(self, html_list):
        # Extract links or identifiers from each HTML snippet
        ...

    def fetch_detail(self, item):
        # Fetch detail page HTML for one record
        ...

    def parse_detail(self, html):
        # Use extract_fields(html, self.config['detail_selectors'])
        ...
```
4.	Run your new crawler: `python run.py --config examples/my_site_config.yaml`

---

## 📂 Project Structure

```
web-crawler-template/
├── core/
│   ├── crawler.py           # BaseCrawler abstract class
│   ├── parser.py            # extract_fields() helper
│   └── utils.py             # rate_limit & retry decorators, logging
├── examples/
│   ├── ai_lit_config.yaml
│   ├── ai_lit_crawler.py
│   ├── climate_lit_config.yaml
│   └── climate_lit_crawler.py
├── data/                    # Output directory (generated by run.py)
├── config.yaml              # Configuration template
├── run.py                   # Command-line entry point
├── requirements.txt
├── README.md
└── LICENSE
```

---

## ⚠️ Disclaimer

- This project is for educational and research purposes only.
- Please respect each target site’s robots.txt and Terms of Service.
- The author assumes no liability for any consequences arising from the use of this code.

---

## 📄 License

Distributed under the [MIT License](LICENSE).
