# Web Crawler Template

A configurable, modular Python web crawler framework.

---

## ğŸš€ Features

- **Modular Core**:  
  - `core/utils.py`: rate-limiting and retry decorators, logging setup  
  - `core/crawler.py`: `BaseCrawler` abstract class defining fetch/parse/save workflow  
  - `core/parser.py`: generic `extract_fields()` helper for CSSâ€selector-based extraction  

- **Config-Driven**:  
  Use a simple YAML file to specify site name, URLs, CSS selectors, concurrency limits, rate limits, and output settings.

- **Example Implementations**:  
  - **AI Litigation Database**: `examples/ai_lit_config.yaml` + `examples/ai_lit_crawler.py`  
  - **Climate Litigation Database**: `examples/climate_lit_config.yaml` + `examples/climate_lit_crawler.py`

- **Easy Extension**:  
  Add a new site by copying the `config.yaml` template and writing a small subclass in `examples/`.

---

## ğŸ“¦ Requirements

```text
pandas>=1.3,<2.0
selenium>=4.0,<5.0
beautifulsoup4>=4.9,<5.0
requests>=2.25,<3.0
PyYAML>=5.3,<6.0
```
Install with: `pip install -r requirements.txt`

---

## ğŸ”§ Quick Start

- Clone the repository: 
     - `git clone https://github.com/<your-username>/web-crawler-template.git
cd web-crawler-template`
- Install dependencies:
     - `pip install -r requirements.txt`
- Run the example crawlers:
     - **AI Litigation Database**: `python run.py --config examples/ai_lit_config.yaml`
     - **Climate Litigation Database**: `python run.py --config examples/climate_lit_config.yaml`
-	Check your output:
     - data/ai_lit.csv
	   - data/climate_lit.xlsx

---

## ğŸ”¨ Adding a New Site

1.	Copy the template: `cp config.yaml my_site_config.yaml`
2.	Edit my_site_config.yaml  
Fill in your siteâ€™s name, list URL, selectors, pagination or load-more settings, and output path.	
3.	Implement a crawler subclass  
Create examples/my_site_crawler.py:
```
from core.crawler import BaseCrawler
from core.parser import extract_fields
from bs4 import BeautifulSoup
import requests

class MySiteCrawler(BaseCrawler):
    def fetch_list(self):
        # Return a list of raw HTML strings or request parameters
        ...

    def parse_list(self, html_list):
        # Extract links or identifiers from each HTML snippet
        ...

    def fetch_detail(self, item):
        # Fetch detail page HTML for one record
        ...

    def parse_detail(self, html):
        # Use extract_fields(html, self.config['detail_selectors'])
        ...
```
4.	Run your new crawler: `python run.py --config examples/my_site_config.yaml`

---

## ğŸ“‚ Project Structure

```
web-crawler-template/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ crawler.py           # BaseCrawler abstract class
â”‚   â”œâ”€â”€ parser.py            # extract_fields() helper
â”‚   â””â”€â”€ utils.py             # rate_limit & retry decorators, logging
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ ai_lit_config.yaml
â”‚   â”œâ”€â”€ ai_lit_crawler.py
â”‚   â”œâ”€â”€ climate_lit_config.yaml
â”‚   â””â”€â”€ climate_lit_crawler.py
â”œâ”€â”€ data/                    # Output directory (generated by run.py)
â”œâ”€â”€ config.yaml              # Configuration template
â”œâ”€â”€ run.py                   # Command-line entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## âš ï¸ Disclaimer

- This project is for educational and research purposes only.
- Please respect each target siteâ€™s robots.txt and Terms of Service.
- The author assumes no liability for any consequences arising from the use of this code.

---

## ğŸ“„ License

Distributed under the [MIT License](LICENSE).
